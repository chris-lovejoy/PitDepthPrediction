{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook demonstrates the use of the function 'run_model_store_JSON' within predict_model.py.\n",
    "\n",
    "The purpose of this function is to take training and test sets, and to (1) report performance, (2) save the model as a .pkl file and (3) record details of the model in JSON file.\n",
    "\n",
    "When designing this function, our intention was for (1) save_model to be toggled off as default, and added if a model is of particular interest and (2) for a single JSON file to be used, where multiple model details can be appended over time.\n",
    "\n",
    "We intended to create an additional function, which extracts information from the JSON file and presents it in an easily intepretable way, but were not able to do so due to time restrictions.\n",
    "\n",
    "\n",
    "### The function takes in the following parameters:\n",
    "- fitted_model: the model that has been trained on the training set\n",
    "- X_train and y_train: the training datasets\n",
    "- X_test and Y-test: the test datasets\n",
    "- dataset_used: the .csv filenmae of the dataset used for training and testing\n",
    "- feature_columns: a list of the column names used for training\n",
    "- JSON_filename: the desired JSON file to record information to\n",
    "- optional_comment: takes a string adding any relevant information\n",
    "    e.g. \"PCA features were created using all phase recordings\"\n",
    "- task_type: default is regression task, can specify 'classification' and different performance metris will be returned\n",
    "- save_model_as_pickle will save a .pkl file of the model in the 'models' folder if True.\n",
    "- scaler: takes the scaler that was used to scale the training set and (1) save it as a pickle file and (2) records the directory for the file in the JSON\n",
    "\n",
    "            \n",
    "The function stores the above in a JSON format, as well as the model performance and the time of training.\n",
    "\n",
    "\n",
    "In this notebook, this function is demonstrated first for a **regression task** and then later for a **classification task**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tube_Alias</th>\n",
       "      <th>Flaw_ID</th>\n",
       "      <th>Angle</th>\n",
       "      <th>Amp_1</th>\n",
       "      <th>Amp_2</th>\n",
       "      <th>Amp_3</th>\n",
       "      <th>Amp_4</th>\n",
       "      <th>Amp_5</th>\n",
       "      <th>Amp_6</th>\n",
       "      <th>Amp_7</th>\n",
       "      <th>...</th>\n",
       "      <th>Phase_15</th>\n",
       "      <th>Phase_16</th>\n",
       "      <th>Phase_17</th>\n",
       "      <th>Phase_18</th>\n",
       "      <th>Phase_19</th>\n",
       "      <th>Phase_20</th>\n",
       "      <th>Flaw_Depth</th>\n",
       "      <th>Pct_Depth</th>\n",
       "      <th>Flaw_Volume</th>\n",
       "      <th>Flaw_Area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>AP01</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>10.320653</td>\n",
       "      <td>14.854606</td>\n",
       "      <td>16.017633</td>\n",
       "      <td>12.423161</td>\n",
       "      <td>23.971155</td>\n",
       "      <td>18.113805</td>\n",
       "      <td>36.668613</td>\n",
       "      <td>...</td>\n",
       "      <td>0.330427</td>\n",
       "      <td>-0.038826</td>\n",
       "      <td>-0.681307</td>\n",
       "      <td>-0.840659</td>\n",
       "      <td>0.556654</td>\n",
       "      <td>0.333518</td>\n",
       "      <td>0.076</td>\n",
       "      <td>10.3</td>\n",
       "      <td>0.864</td>\n",
       "      <td>11.3288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>AP01</td>\n",
       "      <td>A</td>\n",
       "      <td>10</td>\n",
       "      <td>9.256762</td>\n",
       "      <td>13.566036</td>\n",
       "      <td>12.946058</td>\n",
       "      <td>12.594721</td>\n",
       "      <td>19.365281</td>\n",
       "      <td>17.401339</td>\n",
       "      <td>28.518174</td>\n",
       "      <td>...</td>\n",
       "      <td>0.290231</td>\n",
       "      <td>-0.069128</td>\n",
       "      <td>-0.695486</td>\n",
       "      <td>-0.739280</td>\n",
       "      <td>0.548302</td>\n",
       "      <td>0.294714</td>\n",
       "      <td>0.076</td>\n",
       "      <td>10.3</td>\n",
       "      <td>0.864</td>\n",
       "      <td>11.3288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>AP01</td>\n",
       "      <td>A</td>\n",
       "      <td>20</td>\n",
       "      <td>6.375396</td>\n",
       "      <td>8.061063</td>\n",
       "      <td>9.746397</td>\n",
       "      <td>10.879743</td>\n",
       "      <td>14.309066</td>\n",
       "      <td>18.105114</td>\n",
       "      <td>22.923653</td>\n",
       "      <td>...</td>\n",
       "      <td>0.355075</td>\n",
       "      <td>-0.013229</td>\n",
       "      <td>-0.505109</td>\n",
       "      <td>-0.781582</td>\n",
       "      <td>0.626245</td>\n",
       "      <td>0.293746</td>\n",
       "      <td>0.076</td>\n",
       "      <td>10.3</td>\n",
       "      <td>0.864</td>\n",
       "      <td>11.3288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>AP01</td>\n",
       "      <td>A</td>\n",
       "      <td>30</td>\n",
       "      <td>9.700410</td>\n",
       "      <td>11.746437</td>\n",
       "      <td>14.777542</td>\n",
       "      <td>16.300598</td>\n",
       "      <td>18.984767</td>\n",
       "      <td>21.744765</td>\n",
       "      <td>30.582848</td>\n",
       "      <td>...</td>\n",
       "      <td>0.338012</td>\n",
       "      <td>-0.003931</td>\n",
       "      <td>-0.437232</td>\n",
       "      <td>-0.721585</td>\n",
       "      <td>0.560389</td>\n",
       "      <td>0.277560</td>\n",
       "      <td>0.076</td>\n",
       "      <td>10.3</td>\n",
       "      <td>0.864</td>\n",
       "      <td>11.3288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows Ã— 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Tube_Alias Flaw_ID  Angle      Amp_1      Amp_2      Amp_3      Amp_4  \\\n",
       "0       AP01       A      0  10.320653  14.854606  16.017633  12.423161   \n",
       "1       AP01       A     10   9.256762  13.566036  12.946058  12.594721   \n",
       "2       AP01       A     20   6.375396   8.061063   9.746397  10.879743   \n",
       "3       AP01       A     30   9.700410  11.746437  14.777542  16.300598   \n",
       "\n",
       "       Amp_5      Amp_6      Amp_7  ...  Phase_15  Phase_16  Phase_17  \\\n",
       "0  23.971155  18.113805  36.668613  ...  0.330427 -0.038826 -0.681307   \n",
       "1  19.365281  17.401339  28.518174  ...  0.290231 -0.069128 -0.695486   \n",
       "2  14.309066  18.105114  22.923653  ...  0.355075 -0.013229 -0.505109   \n",
       "3  18.984767  21.744765  30.582848  ...  0.338012 -0.003931 -0.437232   \n",
       "\n",
       "   Phase_18  Phase_19  Phase_20  Flaw_Depth  Pct_Depth  Flaw_Volume  Flaw_Area  \n",
       "0 -0.840659  0.556654  0.333518       0.076       10.3        0.864    11.3288  \n",
       "1 -0.739280  0.548302  0.294714       0.076       10.3        0.864    11.3288  \n",
       "2 -0.781582  0.626245  0.293746       0.076       10.3        0.864    11.3288  \n",
       "3 -0.721585  0.560389  0.277560       0.076       10.3        0.864    11.3288  \n",
       "\n",
       "[4 rows x 47 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "path = os.getcwd()\n",
    "new_path = (os.path.join(os.path.dirname(path), 'src'))\n",
    "sys.path.insert(0,new_path)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sklearn\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import data.make_dataset\n",
    "import features.process_data\n",
    "\n",
    "import models.modeling_pipeline\n",
    "from models.modeling_pipeline import get_training_data\n",
    "from models.modeling_pipeline import pick_random_angle_rows\n",
    "from models.train_model import perform_lassoridge_cv\n",
    "\n",
    "from data.make_dataset import CompleteData\n",
    "from data.make_dataset import get_waveform\n",
    "from features.build_features import DataFeatures\n",
    "\n",
    "from features.build_features import extract_PCA_components\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from constants import FilePath, Channel\n",
    "\n",
    "filename = os.path.join(os.path.dirname(os.getcwd()), 'data','interim','full_feature_cg_data.csv')\n",
    "dataframe = pd.read_csv(filename)\n",
    "dataframe.head(4)# to visualise full DataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXAMPLE OF USE 1: FOR LINEAR REGRESSION MODEL GENERATED WITH PCA COMPONENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the columns of features that are general to all future PCA component \n",
    "common_columns = ['Tube_Alias','Flaw_ID', 'Angle','Flaw_Depth', 'Flaw_Volume']\n",
    "selected_feature_columns = dataframe.loc[:,common_columns] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate PCA components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The explained variance (as a ratio) for the 4 principle components (using PhaseDiff if specified) are as follows:\n",
      " [0.60239673 0.12807611 0.09336869 0.06946589]\n",
      "The explained variance (as a ratio) for the 4 principle components (using PhaseAbs if specified) are as follows:\n",
      " [0.32953421 0.18224222 0.10589191 0.07824185]\n",
      "The explained variance (as a ratio) for the 4 principle components (using AmpDiff if specified) are as follows:\n",
      " [9.94252321e-01 5.03896834e-03 6.27853133e-04 3.31979202e-05]\n",
      "The explained variance (as a ratio) for the 4 principle components (using AmpAbs if specified) are as follows:\n",
      " [0.77755723 0.14731867 0.03628239 0.03136264]\n"
     ]
    }
   ],
   "source": [
    "# FOR PHASES\n",
    "differential_phases = ['Phase_1', 'Phase_3', 'Phase_5', 'Phase_7', 'Phase_9', 'Phase_11', 'Phase_13',\n",
    "                                  'Phase_15', 'Phase_17', 'Phase_19']\n",
    "absolute_phases = ['Phase_2', 'Phase_4', 'Phase_6', 'Phase_8', 'Phase_10', 'Phase_12', 'Phase_14',\n",
    "                                  'Phase_16', 'Phase_18', 'Phase_20']\n",
    "\n",
    "components_phases_diff, explained_variance1  = extract_PCA_components(dataframe, differential_phases,\n",
    "                                                num_components=4, comp_names=\"PhaseDiff\")\n",
    "components_phases_abs, explained_variance2 = extract_PCA_components(dataframe, absolute_phases,\n",
    "                                                num_components=4, comp_names=\"PhaseAbs\")\n",
    "\n",
    "pca_dataframe = pd.concat([selected_feature_columns,components_phases_diff,components_phases_abs],axis=1)\n",
    "\n",
    "explained_variance = pd.concat([explained_variance1, explained_variance2],axis=1)\n",
    "\n",
    "\n",
    "# FOR AMPLITUDES\n",
    "differential_amplitudes = ['Amp_1', 'Amp_3', 'Amp_5', 'Amp_7', 'Amp_9', 'Amp_11', 'Amp_13',\n",
    "                                  'Amp_15', 'Amp_17', 'Amp_19']\n",
    "\n",
    "absolute_amplitudes = ['Amp_2', 'Amp_4', 'Amp_6', 'Amp_8', 'Amp_10', 'Amp_12', 'Amp_14',\n",
    "                                  'Amp_16', 'Amp_18', 'Amp_20']\n",
    "\n",
    "components_amps_diff, explained_variance3 = extract_PCA_components(dataframe, differential_amplitudes,\n",
    "                                                num_components=4, comp_names=\"AmpDiff\")\n",
    "\n",
    "components_amps_abs, explained_variance4 = extract_PCA_components(dataframe, absolute_amplitudes,\n",
    "                                                num_components=4, comp_names=\"AmpAbs\")\n",
    "\n",
    "pca_dataframe = pd.concat([pca_dataframe,components_amps_diff,components_amps_abs], axis=1)\n",
    "\n",
    "explained_variance = pd.concat([explained_variance, explained_variance3, explained_variance4],axis=1)\n",
    "\n",
    "# alternative: create separate dataframe for amplitudes\n",
    "# pca_dataframe = pd.concat([selected_feature_columns,components_phases_diff,components_phases_abs],axis=1)a\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select random angle for each pit, to be training samples\n",
    "training_samples = pick_random_angle_rows(pca_dataframe, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT INPUTS AND OUTPUTS - COMMENT IN THE APPROPRIATE ONE\n",
    "\n",
    "inputs = training_samples.drop(common_columns, axis=1) # all of the components\n",
    "# inputs = pca_dataframe[['PhaseDiffComp1','PhaseDiffComp2','PhaseDiffComp3','PhaseDiffComp4']]\n",
    "\n",
    "target_column = 'Flaw_Depth'\n",
    "outputs = training_samples[[target_column]]\n",
    "# outputs = training_samples[['Flaw_Volume']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the percentage of training points to be used for the validation set\n",
    "valid_ratio = 0.15\n",
    "\n",
    "# Define the training and validation samples\n",
    "num_samples = len(training_samples)\n",
    "num_valid = int(np.ceil(num_samples * valid_ratio))\n",
    "num_train = num_samples - num_valid\n",
    "\n",
    "X_train = inputs._slice(slice(num_valid,num_samples)).reset_index().drop(['index'],axis=1)\n",
    "y_train = outputs._slice(slice(num_valid,num_samples)).reset_index().drop(['index'],axis=1)\n",
    "\n",
    "X_valid = inputs._slice(slice(0,num_valid)).reset_index().drop(['index'],axis=1)\n",
    "y_valid = outputs._slice(slice(0,num_valid)).reset_index().drop(['index'],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model \n",
    "model_linear = LinearRegression().fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAVE MODEL AND STORE IN JSON FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = training_samples.drop(common_columns, axis=1).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in models folder as LinearRegression_14-04-2020_11-38-26.\n",
      "\n",
      "              Mean Absolute Error: 0.051093991188801986\n",
      "              Mean Squared Error: 0.004348568776646117\n",
      "              Root Mean Squared Error: 0.0659436788225082\n",
      "              R Squared Score: 0.8902137011681799 \n",
      "              \n",
      "\n",
      "              Mean Absolute Error: 0.08370936386864836\n",
      "              Mean Squared Error: 0.01915870847783486\n",
      "              Root Mean Squared Error: 0.13841498646402006\n",
      "              R Squared Score: 0.5295376522637629 \n",
      "              \n",
      "Record of model and performance stored in temp.txt within models folder.\n"
     ]
    }
   ],
   "source": [
    "from models.predict_model import run_model_store_JSON\n",
    "\n",
    "run_model_store_JSON(model_linear, X_train, y_train, X_valid, y_valid, filename, feature_columns,\n",
    "                     target_column, JSON_filename=\"temp.txt\", task_type =\"regression\",\n",
    "                     save_model = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXAMPLE OF USE 2: FOR CLASSIFICATION MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(filename)\n",
    "\n",
    "# Creating column for whether flaw is through hole (full thickness)\n",
    "df[\"Through_Hole\"] = df['Flaw_ID']\n",
    "df[\"Through_Hole\"] = df[\"Through_Hole\"].map({'A':0,'B':0,'C':0,'D':0,'E':0,'F':0,'G':0,'H':0,'I':1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 18 examples of through hole flaws in the training sample.\n"
     ]
    }
   ],
   "source": [
    "training_samples = pick_random_angle_rows(df, 1)\n",
    "training_samples = shuffle(training_samples)\n",
    "\n",
    "num_through_in_sample = len(training_samples[training_samples['Through_Hole']==1])\n",
    "print(\"There are\", num_through_in_sample, \"examples of through hole flaws in the training sample.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = []\n",
    "for i in range(1,21):\n",
    "    feature_columns.append(f'Amp_{i}')\n",
    "    feature_columns.append(f'Phase_{i}')\n",
    "\n",
    "target_column = 'Through_Hole'   \n",
    "    \n",
    "input_features = training_samples[feature_columns]\n",
    "output_features = training_samples[target_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(input_features, output_features, test_size = 0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.to_numpy().reshape(-1,1)\n",
    "y_test = y_test.to_numpy().reshape(-1,1)\n",
    "\n",
    "\n",
    "sc_x = StandardScaler()\n",
    "X_train = sc_x.fit_transform(X_train)\n",
    "X_test = sc_x.transform(X_test)\n",
    "\n",
    "sc_y = StandardScaler()\n",
    "y_train = sc_y.fit_transform(y_train)\n",
    "y_test = sc_y.transform(y_test)\n",
    "\n",
    "y_train = y_train[:,0]\n",
    "y_test = y_test[:,0]\n",
    "\n",
    "# encode y values as binary, rather than continuous values\n",
    "lab_enc = preprocessing.LabelEncoder()\n",
    "y_train = lab_enc.fit_transform(y_train)\n",
    "y_test = lab_enc.fit_transform(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_estimators=100)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAVE MODEL AND STORE IN JSON FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in models folder as RandomForestClassifier_14-04-2020_11-38-27.\n",
      "Scaler saved in models folder as RandomForestClassifier_14-04-2020_11-38-27_scaler.\n",
      "\n",
      "MODEL PERFORMANCE ON TRAINING SET:\n",
      "\n",
      "              Accuracy: 1.0\n",
      "              AUC: 1.0\n",
      "              F1 score: 1.0\n",
      "              Weighted F1 score: 1.0\n",
      "              Matthew's Correlation Coefficient: 1.0\n",
      "              \n",
      "\n",
      "MODEL PERFORMANCE ON TEST SET:\n",
      "\n",
      "              Accuracy: 0.9705882352941176\n",
      "              AUC: 0.9848484848484849\n",
      "              F1 score: 0.6666666666666666\n",
      "              Weighted F1 score: 0.9752639517345398\n",
      "              Matthew's Correlation Coefficient: 0.6963106238227914\n",
      "              \n",
      "Record of model and performance stored in temp3.txt within models folder.\n"
     ]
    }
   ],
   "source": [
    "from models.predict_model import run_model_store_JSON\n",
    "\n",
    "run_model_store_JSON(model, X_train, y_train, X_test, y_test, filename, feature_columns, \n",
    "                     target_column, JSON_filename=\"temp3.txt\", task_type=\"classification\",\n",
    "                     save_model = True, scaler = sc_x)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
